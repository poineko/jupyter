{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suspended-development",
   "metadata": {},
   "source": [
    "     NAMA  : MUHAMMAD IHSAN AL RIAWI\n",
    "     NPM   : 193510482\n",
    "     KELAS : IV E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proper-balloon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from World.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from World import World\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greater-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def costruct_p(world, p = 0.8, step = -0.04):\n",
    "    nstates = world.get_nstates()\n",
    "    nrows = world.get_nrows()\n",
    "    obsacle_index = world.get_stateobstacles()\n",
    "    terminal_index = world.get_stateterminals()\n",
    "    bad_index = obsacle_index + terminal_index\n",
    "    reward = np.array([step] * 4 + [0] + [step] * 4 + [1, -1] + [step])\n",
    "    actions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    transition_models = {}\n",
    "    for action in actions:\n",
    "        transition_model = np.zeros((nstates, nstates))\n",
    "        for i in range(1, nstates + 1):\n",
    "            if i not in bad_index:\n",
    "                if action == \"N\":\n",
    "                    if i + nrows <= nstates and (i + nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if 0 < i - nrows <= nstates and (i - nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if (i - 1) % nrows > 0 and (i - 1) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - 1 - 1] += p\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                if action == \"S\":\n",
    "                    if i + nrows <= nstates and (i + nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if 0 < i - nrows <= nstates and (i - nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if 0 < i % nrows and (i + 1) not in obsacle_index and (i + 1) <= nstates:\n",
    "                        transition_model[i - 1][i + 1 - 1] += p\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                if action == \"E\":\n",
    "                    if i + nrows <= nstates and (i + nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - nrows - 1] += p\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                    if 0 < i % nrows and (i + 1) not in obsacle_index and (i + 1) <= nstates:\n",
    "                        transition_model[i - 1][i + 1 - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if (i - 1) % nrows > 0 and (i - 1) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - 1 - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                if action == \"W\":\n",
    "                    if 0 < i - nrows <= nstates and (i - nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - nrows - 1] += p\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                    if 0 < i % nrows and (i + 1) not in obsacle_index and (i + 1) <= nstates:\n",
    "                        transition_model[i + 1][i + 1 - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if (i - 1) % nrows > 0 and (i - 1) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - 1 - 1] += (1 - p) / 2\n",
    "                    else :\n",
    "                        transition_model[i - 1][i - 1] += (i - p) / 2\n",
    "            elif i in terminal_index:\n",
    "                transition_model[i - 1][i - 1] = 1\n",
    "        transition_model[action] = pd.DataFrame(transition_model,\n",
    "                                                index = range(1, nstates + 1), columns = range(1, nstates + 1))\n",
    "    return transition_models, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handmade-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_action(transition_models, reward, gamma, s, V, actions, terminal_ind):\n",
    "    maxs = {key: 0 for key in actions}\n",
    "    max_a = \"\"\n",
    "    action_map = {k: v for k, v in zip(actions,[1, 2, 3, 4])}\n",
    "    for action in actions:\n",
    "        if s not in terminal_ind:\n",
    "            maxs[action] += reward[s - 1] + gamma * np.dot(transition_models[action].loc[s, :].values, V)\n",
    "        else :\n",
    "            maxs[action] = reward[s - 1]\n",
    "    maxi = -10 ** 10\n",
    "    for key in maxs:\n",
    "        if maxs[key] > maxi:\n",
    "            max_a = key\n",
    "            maxi = maxs[key]\n",
    "    return maxi, action_map[max_a]\n",
    "\n",
    "def value_iteration(world, transition_models, reward, gamma = 1.0, theta = 10 ** -4):\n",
    "    nstates = world.get_nstates()\n",
    "    terminal_ind = world.get_stateterminals\n",
    "    V = np.zeros((nstates, ))\n",
    "    P = np.zeros((nstates, 1))\n",
    "    actions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    delta = theta + 1\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        v = copy.deepcopy(V)\n",
    "        for s in range(1, nstates + 1):\n",
    "            V[s - 1], P[s - 1] = max_action(transition_models, reward, gamma, s, v, actions, terminal_ind)\n",
    "            delta = max(delta, np.abs(v[s - 1] - V[s - 1]))\n",
    "    return V, P\n",
    "\n",
    "def policy_iter(policy, world, transition_models, reward, gamma = 0.9, theta = 10 ** -4):\n",
    "    nstates = world.get_nstates()\n",
    "    terminal_ind = world.get_stateterminals()\n",
    "    # initiate value function to zeros\n",
    "    V = np.zeros((nstates, ))\n",
    "    a = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # for each state, perform a backup\n",
    "        for s in range(nstates):\n",
    "            v = 0\n",
    "            # look at the policy actions and their probabilities\n",
    "            for action, action_prob in enumerate(policy[s]):\n",
    "                action = a[action]\n",
    "                # for each action, calculate total gain\n",
    "                if s not in terminal_ind:\n",
    "                    v += rewards[s - 1] + action_prob * gamma * np.dot(transition_models[action].loc[s, :].values, V)\n",
    "                else :\n",
    "                    v = rewards[s - 1]\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "            print (V[s])\n",
    "        # stop evaluating once the value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "def policy_improvement(world, transition_models, rewards, gamma = 0.9):\n",
    "    nstates = world.get_nstates()\n",
    "    nActions = world.getnactions()\n",
    "    \n",
    "    # start with a uniform policy\n",
    "    policy = np.ones((nstates, nActions)) / nActions\n",
    "    \n",
    "    while True:\n",
    "        # evaluate the current policy\n",
    "        V = policy_iter(policy, world, transition_models, rewards, gamma)\n",
    "        # will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        for s in range(nstates):\n",
    "            # the best action we would take under the current policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            action_values = lookfoword(s, V, transition_models, rewards, gamma)\n",
    "            best_action = np.argmax(action_values)\n",
    "            \n",
    "            # greedily update the policy\n",
    "            if chosen_a != best_action:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(nActions)[best_action]\n",
    "        if policy_stable:\n",
    "            return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valued-budget",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'World' object has no attribute 'get_nstates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-aa9436f5d6dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# transition_models, rewards = construct_p(world)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Part B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtransition_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcostruct_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransition_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-10f4608820c9>\u001b[0m in \u001b[0;36mcostruct_p\u001b[1;34m(world, p, step)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcostruct_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.04\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mnstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nstates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mobsacle_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stateobstacles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mterminal_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stateterminals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'World' object has no attribute 'get_nstates'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    world = World()\n",
    "    #world.plot()\n",
    "    #world.plot_value([np.random.random() for i in range(12)])\n",
    "    #world.plot_policy(np.random.randint(1, world.nActions, (world.nStates, 1)))\n",
    "    # Part A\n",
    "    # transition_models, rewards = construct_p(world)\n",
    "    # Part B\n",
    "    transition_models, reward = costruct_p(world)\n",
    "    V, P = value_iteration(world, transition_models, rewards)\n",
    "    world.plot_value(V)\n",
    "    world.plot_policy(P)\n",
    "    # Part C\n",
    "    transition_models, rewards = costruct_p(world)\n",
    "    V, P = value_iteration(world, transition_models, rewards, gamma = 0.9)\n",
    "    world.plot_value(V)\n",
    "    world.plot_policy(P)\n",
    "    # Part D\n",
    "    transition_models, rewards = costruct_p(world, step = -0.02)\n",
    "    V, P = value_iteration(world, transition_models, rewards)\n",
    "    world.plot_value(V)\n",
    "    world.plot_policy(P)\n",
    "    # Part E\n",
    "    #transition_models, reward = construct_p(world)\n",
    "    #V, P = value_iteration(world, transition_models, reward, gamma = 0.9)\n",
    "    #world.plot_value(V)\n",
    "    #world.plot_policy(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-obligation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
