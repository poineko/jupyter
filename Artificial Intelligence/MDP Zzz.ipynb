{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floral-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alone-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_states = ['-3', '-2', '-1', '0', '1', '2', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "buried-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_policy = {\n",
    "    '-2': np.array([['l', 'r'],\n",
    "                   [ 0.5, 0.5]]),\n",
    "    '-1': np.array([['l', 'r'],\n",
    "                   [ 0.5, 0.5]]),\n",
    "    '0': np.array([['l', 'r'],\n",
    "                   [ 0.5, 0.5]]),\n",
    "    '1': np.array([['l', 'r'],\n",
    "                   [ 0.5, 0.5]]),\n",
    "    '2': np.array([['l', 'r'],\n",
    "                   [ 0.5, 0.5]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "psychological-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_state_action_probs = {\n",
    "    '-2': {\n",
    "        'l' : np.array([['-3'], [1]]),\n",
    "        'r' : np.array([['-1'], [1]])\n",
    "    },\n",
    "    '-1': {\n",
    "        'l' : np.array([['-2'], [1]]),\n",
    "        'r' : np.array([['0'], [1]])\n",
    "    },\n",
    "    '0': {\n",
    "        'l' : np.array([['-1'], [1]]),\n",
    "        'r' : np.array([['1'], [1]])\n",
    "    },\n",
    "    '1': {\n",
    "        'l' : np.array([['0'], [1]]),\n",
    "        'r' : np.array([['2'], [1]])\n",
    "    },\n",
    "    '2': {\n",
    "        'l' : np.array([['1'], [1]]),\n",
    "        'r' : np.array([['3'], [1]])\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "substantial-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_state_probs = {\n",
    "    '-2': np.array([['-3', '-1'],\n",
    "                   [ 0.5,  0.5]]),\n",
    "    '-1': np.array([['-2', '0'],\n",
    "                   [ 0.5,  0.5]]),\n",
    "    '0': np.array([['-1', '1'],\n",
    "                   [ 0.5,  0.5]]),\n",
    "    '1': np.array([['0', '2'],\n",
    "                   [ 0.5,  0.5]]),\n",
    "    '2': np.array([['1', '3'],\n",
    "                   [ 0.5,  0.5]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faced-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_rewards = {\n",
    "    '-3': -1,\n",
    "    '3': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "express-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess(object):\n",
    "    def __init__(self, states, state_transition_probabilities, initial_state=None):\n",
    "        self.states = states\n",
    "        self.state_transition_probabilities = state_transition_probabilities\n",
    "        self.initial_state = self.states[0]\n",
    "        if initial_state is not None:\n",
    "            self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        \n",
    "        self.terminal_states = []\n",
    "        for state in self.states:\n",
    "            if state in self.state_transition_probabilities.keys():\n",
    "                p = self.state_transition_probabilities[state]\n",
    "                if len(p[0]) == 1 and p[0][0] == state:\n",
    "                    # If a state transitions only to itself, it is a terminal state\n",
    "                    self.terminal_states.append(state)\n",
    "            else:\n",
    "                # If a state has no probabilities defined, treat it as terminal\n",
    "                self.terminal_states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "empty-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "        states = '(' + ', '.join(self.states) + ')'\n",
    "        probs = \"(...)\"\n",
    "\n",
    "        return \"Markov Process with S={}, Ps={}, S_t={:+d}\".format(\n",
    "            states,\n",
    "            probs,\n",
    "            int(self.state)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "trained-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self):\n",
    "\n",
    "        # Transition to new state\n",
    "        tmp = self.state_transition_probabilities[self.state]\n",
    "        self.state = np.random.choice(a=list(tmp[0]), p=list(tmp[1]))\n",
    "\n",
    "        # Increment step counter\n",
    "        self.num_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smart-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    " def is_terminated(self):\n",
    "        return self.state in self.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exciting-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(self, verbose=False):\n",
    "        # Perform a full rollout of the Markov Process\n",
    "        while not self.is_terminated():\n",
    "            self.step()\n",
    "            if verbose is True:\n",
    "                print(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "traditional-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self):\n",
    "        self.state = self.initial_state\n",
    "        self.num_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tight-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess(MarkovProcess):\n",
    "    def __init__(self, states, state_transition_probabilities, rewards=None, discount=1, initial_state=None):\n",
    "\n",
    "        # Call Markov Process constructor\n",
    "        super().__init__(states, state_transition_probabilities, initial_state)\n",
    "\n",
    "        # Store states\n",
    "        self.rewards = rewards\n",
    "\n",
    "        # Store discount\n",
    "        self.discount = discount\n",
    "\n",
    "        # Initialize return\n",
    "        self._return = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        states = '(' + ', '.join(self.states) + ')'\n",
    "        probs = \"(...)\"\n",
    "        rewards = \"(...)\"\n",
    "\n",
    "        return \"Markov Reward Process with S={}, Psa={}, R={}, g={:.2f}, S_t={:+d}, G_t={:+d}\".format(\n",
    "            states,\n",
    "            probs,\n",
    "            rewards,\n",
    "            self.discount,\n",
    "            int(self.state),\n",
    "            self._return\n",
    "        )\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        # Call Markov Process step\n",
    "        super().step()\n",
    "\n",
    "        # Check if we got any reward\n",
    "        if self.rewards is not None:\n",
    "            self._return += self.rewards.get(self.state, 0)\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # Call Markov Process reset\n",
    "        super().reset()\n",
    "\n",
    "        self._return = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "material-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess(MarkovRewardProcess):\n",
    "     def __init__(\n",
    "        self,\n",
    "        states,\n",
    "        policy,\n",
    "        state_action_transition_probabilities,\n",
    "        rewards=None,\n",
    "        discount=1,\n",
    "        initial_state=None\n",
    "        ):\n",
    "\n",
    "        # TODO ajs 24/Jan/2017 Unroll the decision process under the given policy into a\n",
    "        # reward process, then call Markov Reward Process constructor\n",
    "        #super().__init__(states, transition_probabilities, initial_state)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "addressed-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "        states = '(' + ', '.join(self.states) + ')'\n",
    "        policy = \"(...)\"\n",
    "        probs = \"(...)\"\n",
    "        rewards = \"(...)\"\n",
    "\n",
    "        return \"Markov Decision Process with S={}, pi={}, Psa={}, R={}, g={:.2f}, S_t={:+d}, G_t={:+d}\".format(\n",
    "            states,\n",
    "            policy,\n",
    "            probs,\n",
    "            rewards,\n",
    "            self.discount,\n",
    "            int(self.state),\n",
    "            self._return\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "classical-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self):\n",
    "        # TODO: Sample policy, then let environment go\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "allied-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #mp = MarkovProcess(rw_states, rw_state_probs, initial_state='0')\n",
    "    mp = MarkovRewardProcess(rw_states, rw_state_probs, rewards=rw_rewards, initial_state='0')\n",
    "\n",
    "    average_num_steps = 0\n",
    "    average_return = 0\n",
    "    N = 10\n",
    "    for i in range(N):\n",
    "        mp.reset()\n",
    "        mp.rollout(True)\n",
    "        average_num_steps += mp.num_steps\n",
    "        average_return += mp._return\n",
    "\n",
    "    average_num_steps /= N\n",
    "    average_return /= N\n",
    "\n",
    "    print(average_num_steps)\n",
    "    print(average_return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "palestinian-newsletter",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-ac204f12c966>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0maverage_num_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-ffef9b2e7986>\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# Call Markov Process reset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
