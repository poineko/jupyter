{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ongoing-challenge",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wrd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-45ec290e69f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wrd'"
     ]
    }
   ],
   "source": [
    "from wrd import wrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "\n",
    "def construct_p(world, p=0.8, step=-0.04):\n",
    "\n",
    "    nstates = world.get_nstates()\n",
    "    nrows = world.get_nrows()\n",
    "    obsacle_index = world.get_stateobstacles()\n",
    "    terminal_index = world.get_stateterminals()\n",
    "    bad_index = obsacle_index + terminal_index\n",
    "    rewards = np.array([step] * 4 + [0] + [step] * 4 + [1, -1] + [step])\n",
    "    actions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    transition_models = {}\n",
    "    for action in actions:\n",
    "        transition_model = np.zeros((nstates, nstates))\n",
    "        for i in range(1, nstates + 1):\n",
    "            if i not in bad_index:\n",
    "                if action == \"N\":\n",
    "                    if i + nrows <= nstates and (i + nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if 0 < i - nrows <= nstates and (i - nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - nrows - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if (i - 1) % nrows > 0 and (i - 1) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - 1 - 1] += p\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                if action == \"S\":\n",
    "                    if i + nrows <= nstates and (i + nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if 0 < i - nrows <= nstates and (i - nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - nrows - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if 0 < i % nrows and (i + 1) not in obsacle_index and (i + 1) <= nstates:\n",
    "                        transition_model[i - 1][i + 1 - 1] += p\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                if action == \"E\":\n",
    "                    if i + nrows <= nstates and (i + nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i + nrows - 1] += p\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                    if 0 < i % nrows and (i + 1) not in obsacle_index and (i + 1) <= nstates:\n",
    "                        transition_model[i - 1][i + 1 - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if (i - 1) % nrows > 0 and (i - 1) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - 1 - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                if action == \"W\":\n",
    "                    if 0 < i - nrows <= nstates and (i - nrows) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - nrows - 1] += p\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += p\n",
    "                    if 0 < i % nrows and (i + 1) not in obsacle_index and (i + 1) <= nstates:\n",
    "                        transition_model[i - 1][i + 1 - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "                    if (i - 1) % nrows > 0 and (i - 1) not in obsacle_index:\n",
    "                        transition_model[i - 1][i - 1 - 1] += (1 - p) / 2\n",
    "                    else:\n",
    "                        transition_model[i - 1][i - 1] += (1 - p) / 2\n",
    "            elif i in terminal_index:\n",
    "                transition_model[i - 1][i - 1] = 1\n",
    "        transition_models[action] = pd.DataFrame(transition_model, index=range(1, nstates + 1), columns=range(1, nstates + 1))\n",
    "\n",
    "    return transition_models, rewards\n",
    "\n",
    "\n",
    "def max_action(transition_models, rewards, gamma, s, V, actions, terminal_ind):\n",
    "\n",
    "    maxs = {key: 0 for key in actions}\n",
    "    max_a = \"\"\n",
    "    action_map = {k: v for k, v in zip(actions, [1, 3, 2, 4])}\n",
    "    for action in actions:\n",
    "        if s not in terminal_ind:\n",
    "            maxs[action] += rewards[s - 1] + gamma * np.dot(transition_models[action].loc[s, :].values, V)\n",
    "        else:\n",
    "            maxs[action] = rewards[s - 1]\n",
    "    maxi = -10 ** 10\n",
    "    for key in maxs:\n",
    "        if maxs[key] > maxi:\n",
    "            max_a = key\n",
    "            maxi = maxs[key]\n",
    "    return maxi, action_map[max_a]\n",
    "\n",
    "\n",
    "def value_iteration(world, transition_models, rewards, gamma=1.0, theta=10 ** -4):\n",
    "\n",
    "    nstates = world.get_nstates()\n",
    "    terminal_ind = world.get_stateterminals()\n",
    "    V = np.zeros((nstates, ))\n",
    "    P = np.zeros((nstates, 1))\n",
    "    actions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    delta = theta + 1\n",
    "    while delta > theta:\n",
    "        delta = 0\n",
    "        v = copy.deepcopy(V)\n",
    "        for s in range(1, nstates + 1):\n",
    "            V[s - 1], P[s - 1] = max_action(transition_models, rewards, gamma, s, v, actions, terminal_ind)\n",
    "            delta = max(delta, np.abs(v[s - 1] - V[s - 1]))\n",
    "    return V, P\n",
    "\n",
    "\n",
    "def policy_iter(policy, world, transition_models, rewards, gamma=0.9, theta=10 ** -4):\n",
    "\n",
    "    nstates = world.get_nstates()\n",
    "    terminal_ind = world.get_stateterminals()\n",
    "    # Initiate value function to zeros\n",
    "    V = np.zeros((nstates,))\n",
    "    a = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a backup\n",
    "        for s in range(nstates):\n",
    "            v = 0\n",
    "            # Look at the policy actions and their probabilities\n",
    "            for action, action_prob in enumerate(policy[s]):\n",
    "                action = a[action]\n",
    "                # For each action, calculate total gain\n",
    "                if s not in terminal_ind:\n",
    "                    v += rewards[s - 1] + action_prob * gamma * np.dot(transition_models[action].loc[s, :].values, V)\n",
    "                else:\n",
    "                    v = rewards[s - 1]\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "            print (V[s])\n",
    "        # Stop evaluating once the value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "#  Helper function to calculate the value for all action in a given state\n",
    "def lookfoword(s, V, transition_models, rewards, gamma = 0.9):\n",
    "\n",
    "    nActions = world.get_nstates()\n",
    "    terminal_ind = world.get_stateterminals()\n",
    "    A = np.zeros(nActions)\n",
    "    a = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    for i, action in enumerate(nActions):\n",
    "        action = a[action]\n",
    "        if s not in terminal_ind:\n",
    "            A[i] += rewards[s - 1] + gamma * np.dot(transition_models[a].loc[s, :].values, V)\n",
    "        else:\n",
    "            A[i] = rewards[s - 1]\n",
    "    return A\n",
    "\n",
    "\n",
    "def policy_improvement(world, transition_models, rewards, gamma= 0.9):\n",
    "\n",
    "    nstates = world.get_nstates()\n",
    "    nActions = world.get_nactions()\n",
    "\n",
    "\n",
    "    # Start with a uniform policy\n",
    "    policy = np.ones((nstates, nActions)) / nActions\n",
    "\n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_iter(policy, world, transition_models, rewards, gamma)\n",
    "\n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in range(nstates):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "\n",
    "            action_values = lookfoword(s, V, transition_models, rewards, gamma)\n",
    "            best_action = np.argmax(action_values)\n",
    "\n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_action:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(nActions)[best_action]\n",
    "\n",
    "        if policy_stable:\n",
    "            return V, policy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    world = World()\n",
    "    # world.plot()\n",
    "    # world.plot_value([np.random.random() for i in range(12)])\n",
    "    # world.plot_policy(np.random.randint(1, world.nActions,(world.nStates, 1)))\n",
    "    # part a\n",
    "    # transition_models, rewards = construct_p(world)\n",
    "    # part b\n",
    "    transition_models, rewards = construct_p(world)\n",
    "    V, P = value_iteration(world, transition_models, rewards)\n",
    "    world.plot_value(V)\n",
    "    world.plot_policy(P)\n",
    "    # part c\n",
    "    transition_models, rewards = construct_p(world)\n",
    "    V, P = value_iteration(world, transition_models, rewards, gamma=0.9)\n",
    "    world.plot_value(V)\n",
    "    world.plot_policy(P)\n",
    "    # part d\n",
    "    transition_models, rewards = construct_p(world, step=-0.02)\n",
    "    V, P = value_iteration(world, transition_models, rewards)\n",
    "    world.plot_value(V)\n",
    "    world.plot_policy(P)\n",
    "    # part e\n",
    "    # transition_models, rewards = construct_p(world)\n",
    "    # V, P = policy_improvement(world, transition_models, rewards, gamma=0.9)\n",
    "    # world.plot_value(V)\n",
    "    # world.plot_policy(P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-section",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
