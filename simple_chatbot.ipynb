{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "valuable-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"data/intents.json\") as data_file:\n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "formed-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = []\n",
    "intents = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        text_input.append(pattern)\n",
    "        intents.append(intent['tag'])\n",
    "        \n",
    "df = pd.DataFrame({'text_input': text_input,\n",
    "                   'intents': intents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closed-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_input</th>\n",
       "      <th>intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hai</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halo</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apa Kabar</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Selamat Pagi</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_input intents\n",
       "0           Hai   salam\n",
       "1            Hi   salam\n",
       "2          Halo   salam\n",
       "3     Apa Kabar   salam\n",
       "4  Selamat Pagi   salam"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "purple-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salam         11\n",
       "bye            8\n",
       "nama           6\n",
       "kemampuan      5\n",
       "KUA            5\n",
       "pekerjaan      5\n",
       "Berkas         3\n",
       "pernikahan     3\n",
       "rujuk          3\n",
       "Name: intents, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.intents.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reverse-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df.text_input = df.text_input.apply(lambda x: x.lower())\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "df.text_input = df.text_input.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medieval-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df.intents)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enhanced-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = []\n",
    "length = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    sent = row['text_input']\n",
    "    [all_vocab.append(i) for i in sent.split()]\n",
    "    length.append(len(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adjustable-tribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liberal-bennett",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apparent-possible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "circular-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "max_vocab_length = 86\n",
    "max_length = 6\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                       standardize='lower_and_strip_punctuation',\n",
    "                                       split='whitespace',\n",
    "                                       ngrams=None,\n",
    "                                       output_mode='int',\n",
    "                                       output_sequence_length=max_length\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cutting-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(df.text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mature-impact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'apa',\n",
       " 'kua',\n",
       " 'aja',\n",
       " 'lo',\n",
       " 'kamu',\n",
       " 'yang',\n",
       " 'selamat',\n",
       " 'ngapain',\n",
       " 'mau',\n",
       " 'lu',\n",
       " 'bisa',\n",
       " 'siapa',\n",
       " 'rujuk',\n",
       " 'nama',\n",
       " 'kalo',\n",
       " 'emang',\n",
       " 'tuh',\n",
       " 'tugas',\n",
       " 'sih',\n",
       " 'sape',\n",
       " 'ok',\n",
       " 'gimana',\n",
       " 'di',\n",
       " 'dah',\n",
       " 'bawa',\n",
       " 'wah',\n",
       " 'tinggal',\n",
       " 'syarat2',\n",
       " 'siang',\n",
       " 'si',\n",
       " 'semoga',\n",
       " 'saya',\n",
       " 'sampai',\n",
       " 'salam',\n",
       " 'ping',\n",
       " 'permisi',\n",
       " 'perlu',\n",
       " 'pekerjaan',\n",
       " 'pagi',\n",
       " 'p',\n",
       " 'nya',\n",
       " 'nikah',\n",
       " 'menyenangkan',\n",
       " 'menikah',\n",
       " 'malam',\n",
       " 'makasih',\n",
       " 'lakukan',\n",
       " 'lagi',\n",
       " 'kerja',\n",
       " 'kemampuan',\n",
       " 'ke',\n",
       " 'kabar',\n",
       " 'jumpa',\n",
       " 'hi',\n",
       " 'harus',\n",
       " 'harimu',\n",
       " 'halo',\n",
       " 'hai',\n",
       " 'dibawa',\n",
       " 'dadah',\n",
       " 'daah',\n",
       " 'bye',\n",
       " 'berkas',\n",
       " 'anda']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "living-supervision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int64, numpy=array([58, 33, 10, 45,  0,  0], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization('halo saya mau menikah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "protecting-assignment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "regular-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding = Embedding(input_dim=max_vocab_length,\n",
    "                      output_dim=16,\n",
    "                      embeddings_initializer=\"uniform\",\n",
    "                      input_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "israeli-causing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 16), dtype=float32, numpy=\n",
       "array([[[ 0.04194237,  0.0130688 ,  0.01129778, -0.01781829,\n",
       "         -0.04763865,  0.03290497, -0.02823228,  0.00031004,\n",
       "         -0.02198743, -0.00862699, -0.03788847,  0.02357319,\n",
       "         -0.04832486,  0.0405406 ,  0.04538328,  0.03169156],\n",
       "        [-0.03514776,  0.04857652, -0.01164766, -0.0059258 ,\n",
       "          0.03125368,  0.01332663,  0.00911186, -0.03085229,\n",
       "          0.03050241, -0.03810525,  0.03780134, -0.01485773,\n",
       "          0.02375335,  0.04321754,  0.01261162,  0.01931432],\n",
       "        [-0.00240798,  0.04827963,  0.04958204, -0.02234999,\n",
       "         -0.00779647,  0.03184755, -0.00563807, -0.01604617,\n",
       "          0.04825505, -0.03456446, -0.02288265, -0.01593714,\n",
       "          0.04310013, -0.00767217, -0.01973579, -0.04475765],\n",
       "        [ 0.04593131,  0.01069583,  0.04892487,  0.04448426,\n",
       "         -0.04215707, -0.02442079, -0.0492838 ,  0.00950613,\n",
       "          0.00554507, -0.04543059, -0.04565214,  0.02983906,\n",
       "          0.03018907, -0.02684227,  0.03969424,  0.01315037],\n",
       "        [-0.03758453, -0.03402256, -0.00666402,  0.00262104,\n",
       "          0.03024589, -0.00826352,  0.04547757,  0.04827927,\n",
       "         -0.03684795,  0.01969339,  0.02751889,  0.02578804,\n",
       "          0.02717689,  0.00802946, -0.02869667,  0.01426225],\n",
       "        [-0.03758453, -0.03402256, -0.00666402,  0.00262104,\n",
       "          0.03024589, -0.00826352,  0.04547757,  0.04827927,\n",
       "         -0.03684795,  0.01969339,  0.02751889,  0.02578804,\n",
       "          0.02717689,  0.00802946, -0.02869667,  0.01426225]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "res_embed = embedding(np.array([[58, 33, 10, 45,  0,  0]]))\n",
    "res_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arctic-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "inputs = Input(shape=(1,), dtype='string')\n",
    "x = text_vectorization(inputs)\n",
    "x = embedding(x)\n",
    "x = LSTM(12)(x)\n",
    "outputs = Dense(9, activation='softmax')(x)\n",
    "model_lstm = Model(inputs, outputs, name=\"LSTM_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sufficient-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "blond-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15377353820>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(df.text_input,\n",
    "               y_train,\n",
    "               epochs=200,\n",
    "               verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "proved-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 24ms/step - loss: 0.3025 - accuracy: 0.9592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3025497496128082, 0.9591836929321289]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.evaluate(df.text_input, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bigger-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bot_model.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bot_model.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "model_lstm.save(\"bot_model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "shared-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "le_filename = open(\"label_encoder.pickle\", \"wb\")\n",
    "pickle.dump(le, le_filename)\n",
    "le_filename.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
